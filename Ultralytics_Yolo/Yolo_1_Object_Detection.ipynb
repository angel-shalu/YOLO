{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOT_GXbcMd4b",
        "outputId": "0ba9ecd3-bbdd-4c6a-9723-480ffeec398b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.218  Python-3.13.3 torch-2.9.0+cpu CPU (12th Gen Intel Core i5-1235U)\n",
            "Setup complete  (12 CPUs, 15.7 GB RAM, 241.4/426.1 GB disk)\n"
          ]
        }
      ],
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xecTPvUtND0N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.218  Python-3.13.3 torch-2.9.0+cpu CPU (12th Gen Intel Core i5-1235U)\n",
            "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
            "\n",
            "Found https://ultralytics.com/images/zidane.jpg locally at zidane.jpg\n",
            "image 1/1 c:\\Users\\shali\\Desktop\\DS_Road_Map\\14.YOLO\\Ultralytics_Yolo\\zidane.jpg: 384x640 2 persons, 1 tie, 143.6ms\n",
            "Speed: 5.5ms preprocess, 143.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Results saved to \u001b[1mC:\\Users\\shali\\Desktop\\DS_Road_Map\\14.YOLO\\Ultralytics_Yolo\\runs\\detect\\predict2\u001b[0m\n",
            " Learn more at https://docs.ultralytics.com/modes/predict\n"
          ]
        }
      ],
      "source": [
        "# Run inference on an image with YOLO11n\n",
        "!yolo predict model=yolo11n.pt source='https://ultralytics.com/images/zidane.jpg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gGcUZ2KNDpW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.218  Python-3.13.3 torch-2.9.0+cpu CPU (12th Gen Intel Core i5-1235U)\n",
            "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \u001b[35m\"<frozen runpy>\"\u001b[0m, line \u001b[35m198\u001b[0m, in \u001b[35m_run_module_as_main\u001b[0m\n",
            "  File \u001b[35m\"<frozen runpy>\"\u001b[0m, line \u001b[35m88\u001b[0m, in \u001b[35m_run_code\u001b[0m\n",
            "  File \u001b[35m\"c:\\Users\\shali\\AppData\\Roaming\\Python\\Python313\\Scripts\\yolo.exe\\__main__.py\"\u001b[0m, line \u001b[35m6\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
            "    sys.exit(\u001b[31mentrypoint\u001b[0m\u001b[1;31m()\u001b[0m)\n",
            "             \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "  File \u001b[35m\"C:\\Users\\shali\\AppData\\Roaming\\Python\\Python313\\site-packages\\ultralytics\\cfg\\__init__.py\"\u001b[0m, line \u001b[35m990\u001b[0m, in \u001b[35mentrypoint\u001b[0m\n",
            "    \u001b[31mgetattr(model, mode)\u001b[0m\u001b[1;31m(**overrides)\u001b[0m  # default args from model\n",
            "    \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"C:\\Users\\shali\\AppData\\Roaming\\Python\\Python313\\site-packages\\ultralytics\\engine\\model.py\"\u001b[0m, line \u001b[35m557\u001b[0m, in \u001b[35mpredict\u001b[0m\n",
            "    return \u001b[31mself.predictor.predict_cli\u001b[0m\u001b[1;31m(source=source)\u001b[0m if is_cli else self.predictor(source=source, stream=stream)\n",
            "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"C:\\Users\\shali\\AppData\\Roaming\\Python\\Python313\\site-packages\\ultralytics\\engine\\predictor.py\"\u001b[0m, line \u001b[35m249\u001b[0m, in \u001b[35mpredict_cli\u001b[0m\n",
            "    for _ in \u001b[1;31mgen\u001b[0m:  # sourcery skip: remove-empty-nested-block, noqa\n",
            "             \u001b[1;31m^^^\u001b[0m\n",
            "  File \u001b[35m\"C:\\Users\\shali\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\_contextlib.py\"\u001b[0m, line \u001b[35m38\u001b[0m, in \u001b[35mgenerator_context\u001b[0m\n",
            "    response = gen.send(None)\n",
            "  File \u001b[35m\"C:\\Users\\shali\\AppData\\Roaming\\Python\\Python313\\site-packages\\ultralytics\\engine\\predictor.py\"\u001b[0m, line \u001b[35m306\u001b[0m, in \u001b[35mstream_inference\u001b[0m\n",
            "    \u001b[31mself.setup_source\u001b[0m\u001b[1;31m(source if source is not None else self.args.source)\u001b[0m\n",
            "    \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"C:\\Users\\shali\\AppData\\Roaming\\Python\\Python313\\site-packages\\ultralytics\\engine\\predictor.py\"\u001b[0m, line \u001b[35m261\u001b[0m, in \u001b[35msetup_source\u001b[0m\n",
            "    self.dataset = \u001b[31mload_inference_source\u001b[0m\u001b[1;31m(\u001b[0m\n",
            "                   \u001b[31m~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
            "        \u001b[1;31msource=source,\u001b[0m\n",
            "        \u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
            "    ...<3 lines>...\n",
            "        \u001b[1;31mchannels=getattr(self.model, \"ch\", 3),\u001b[0m\n",
            "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "    \u001b[1;31m)\u001b[0m\n",
            "    \u001b[1;31m^\u001b[0m\n",
            "  File \u001b[35m\"C:\\Users\\shali\\AppData\\Roaming\\Python\\Python313\\site-packages\\ultralytics\\data\\build.py\"\u001b[0m, line \u001b[35m424\u001b[0m, in \u001b[35mload_inference_source\u001b[0m\n",
            "    dataset = LoadImagesAndVideos(source, batch=batch, vid_stride=vid_stride, channels=channels)\n",
            "  File \u001b[35m\"C:\\Users\\shali\\AppData\\Roaming\\Python\\Python313\\site-packages\\ultralytics\\data\\loaders.py\"\u001b[0m, line \u001b[35m376\u001b[0m, in \u001b[35m__init__\u001b[0m\n",
            "    raise FileNotFoundError(f\"{p} does not exist\")\n",
            "\u001b[1;35mFileNotFoundError\u001b[0m: \u001b[35m'C:\\Users\\shali\\Desktop\\DS_Road_Map\\14.YOLO\\Ultralytics_Yolo\\1.jpg' does not exist\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Run inference on an image with YOLO11n\n",
        "!yolo predict model=yolo11n.pt source='https://images.picxy.com/cache/2019/7/25/7cb6f63a656f23f38c4fff9a4cf219bc.jpg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
